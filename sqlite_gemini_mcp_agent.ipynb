{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQfyjpFxnuvU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruW-6e4StLbh",
        "outputId": "d69c3e61-51c1-4ce7-9ba3-9b57ea3a420f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mcp==1.1.2\n",
            "  Downloading mcp-1.1.2-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting anyio>=4.5 (from mcp==1.1.2)\n",
            "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting httpx-sse>=0.4 (from mcp==1.1.2)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.11/dist-packages (from mcp==1.1.2) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.7.2 in /usr/local/lib/python3.11/dist-packages (from mcp==1.1.2) (2.10.6)\n",
            "Collecting sse-starlette>=1.6.1 (from mcp==1.1.2)\n",
            "  Downloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting starlette>=0.27 (from mcp==1.1.2)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp==1.1.2) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp==1.1.2) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio>=4.5->mcp==1.1.2) (4.12.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->mcp==1.1.2) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27->mcp==1.1.2) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27->mcp==1.1.2) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.2->mcp==1.1.2) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.2->mcp==1.1.2) (2.27.2)\n",
            "Downloading mcp-1.1.2-py3-none-any.whl (36 kB)\n",
            "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: httpx-sse, anyio, starlette, sse-starlette, mcp\n",
            "  Attempting uninstall: anyio\n",
            "    Found existing installation: anyio 3.7.1\n",
            "    Uninstalling anyio-3.7.1:\n",
            "      Successfully uninstalled anyio-3.7.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-server 1.24.0 requires anyio<4,>=3.1.0, but you have anyio 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed anyio-4.9.0 httpx-sse-0.4.0 mcp-1.1.2 sse-starlette-2.2.1 starlette-0.46.1\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install mcp==1.1.2\n",
        "!pip install openai\n",
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "n-O98Sd98RKc"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from huggingface_hub import get_token\n",
        "from mcp import ClientSession, StdioServerParameters\n",
        "from mcp.client.stdio import stdio_client\n",
        "from typing import Any, List\n",
        "import asyncio\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.genai.chats import Chat\n",
        "\n",
        "\n",
        "MODEL_ID = \"gemini-2.0-flash-exp\"\n",
        "\n",
        "# System prompt that guides the LLM's behavior and capabilities\n",
        "SYSTEM_PROMPT = \"\"\"You are a helpful assistant capable of accessing external functions and engaging in casual chat. Use the responses from these function calls to provide accurate and informative answers. The answers should be natural and hide the fact that you are using tools to access real-time information. Guide the user about available tools and their capabilities. Always utilize tools to access real-time information when required. Engage in a friendly manner to enhance the chat experience.\n",
        "\n",
        "# Tools\n",
        "\n",
        "{tools}\n",
        "\n",
        "# Notes\n",
        "\n",
        "- Ensure responses are based on the latest information available from function calls.\n",
        "- Maintain an engaging, supportive, and friendly tone throughout the dialogue.\n",
        "- Always highlight the potential of available tools to assist users comprehensively.\"\"\"\n",
        "\n",
        "\n",
        "# Initialize client using AI Studio API key\n",
        "api_key = os.getenv(\"GOOGLE_API_KEY\", \"PLACE_YOUR_API_KEY_HERE\" )\n",
        "client = genai.Client(api_key=api_key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z_BD2IcWnvh2"
      },
      "outputs": [],
      "source": [
        "class MCPClient:\n",
        "    \"\"\"\n",
        "    A client class for interacting with the MCP (Model Control Protocol) server.\n",
        "    This class manages the connection and communication with the SQLite database through MCP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, server_params: StdioServerParameters):\n",
        "        \"\"\"Initialize the MCP client with server parameters\"\"\"\n",
        "        self.server_params = server_params\n",
        "        self.session = None\n",
        "        self._client = None\n",
        "\n",
        "    async def __aenter__(self):\n",
        "        \"\"\"Async context manager entry\"\"\"\n",
        "        await self.connect()\n",
        "        return self\n",
        "\n",
        "    async def __aexit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Async context manager exit\"\"\"\n",
        "        if self.session:\n",
        "            await self.session.__aexit__(exc_type, exc_val, exc_tb)\n",
        "        if self._client:\n",
        "            await self._client.__aexit__(exc_type, exc_val, exc_tb)\n",
        "\n",
        "    async def connect(self):\n",
        "        \"\"\"Establishes connection to MCP server\"\"\"\n",
        "        self._client = stdio_client(self.server_params)\n",
        "        self.read, self.write = await self._client.__aenter__()\n",
        "        session = ClientSession(self.read, self.write)\n",
        "        self.session = await session.__aenter__()\n",
        "        await self.session.initialize()\n",
        "\n",
        "    async def get_available_tools(self) -> List[Any]:\n",
        "        \"\"\"\n",
        "        Retrieve a list of available tools from the MCP server.\n",
        "        \"\"\"\n",
        "        if not self.session:\n",
        "            raise RuntimeError(\"Not connected to MCP server\")\n",
        "\n",
        "        tools = await self.session.list_tools()\n",
        "        _, tools_list = tools\n",
        "        _, tools_list = tools_list\n",
        "        return tools_list\n",
        "\n",
        "    def call_tool(self, tool_name: str) -> Any:\n",
        "        \"\"\"\n",
        "        Create a callable function for a specific tool.\n",
        "        This allows us to execute database operations through the MCP server.\n",
        "\n",
        "        Args:\n",
        "            tool_name: The name of the tool to create a callable for\n",
        "\n",
        "        Returns:\n",
        "            A callable async function that executes the specified tool\n",
        "        \"\"\"\n",
        "        if not self.session:\n",
        "            raise RuntimeError(\"Not connected to MCP server\")\n",
        "\n",
        "        async def callable(*args, **kwargs):\n",
        "            response = await self.session.call_tool(tool_name, arguments=kwargs)\n",
        "            return response.content[0].text\n",
        "\n",
        "        return callable\n",
        "\n",
        "\n",
        "async def agent_loop(query: str, tools: dict, messages: List[types.Content] = None):\n",
        "    \"\"\"\n",
        "    Main interaction loop that processes user queries using the LLM and available tools.\n",
        "\n",
        "    This function:\n",
        "    1. Sends the user query to the LLM with context about available tools\n",
        "    2. Processes the LLM's response, including any tool calls\n",
        "    3. Returns the final response to the user\n",
        "\n",
        "    Args:\n",
        "        query: User's input question or command\n",
        "        tools: Dictionary of available database tools and their schemas\n",
        "        messages: List of messages to pass to the LLM, defaults to None\n",
        "    \"\"\"\n",
        "    # Convert tools to Gemini function declarations format\n",
        "    tool_declarations = []\n",
        "    for tool in tools.values():\n",
        "        # dirty way to convert the types to Gemini compatible types\n",
        "        parsed_parameters = json.loads(\n",
        "            json.dumps(tool[\"schema\"][\"function\"][\"parameters\"])\n",
        "            .replace(\"object\", \"OBJECT\")\n",
        "            .replace(\"string\", \"STRING\")\n",
        "            .replace(\"number\", \"NUMBER\")\n",
        "            .replace(\"boolean\", \"BOOLEAN\")\n",
        "            .replace(\"array\", \"ARRAY\")\n",
        "            .replace(\"integer\", \"INTEGER\")\n",
        "        )\n",
        "        declaration = types.FunctionDeclaration(\n",
        "            name=tool[\"name\"],\n",
        "            description=tool[\"schema\"][\"function\"][\"description\"],\n",
        "            parameters=parsed_parameters,\n",
        "        )\n",
        "        tool_declarations.append(declaration)\n",
        "\n",
        "    # Initialize chat with system instruction\n",
        "    generation_config = types.GenerateContentConfig(\n",
        "        system_instruction=SYSTEM_PROMPT.format(\n",
        "            tools=\"\\n- \".join(\n",
        "                [\n",
        "                    f\"{t['name']}: {t['schema']['function']['description']}\"\n",
        "                    for t in tools.values()\n",
        "                ]\n",
        "            )\n",
        "        ),\n",
        "        temperature=0,\n",
        "        tools=[types.Tool(function_declarations=tool_declarations)],\n",
        "    )\n",
        "    contents = [] if messages is None else messages # check if there is a previous conversation\n",
        "    contents.append(types.Content(role=\"user\", parts=[types.Part(text=query)])) # add the user query to the contents\n",
        "    # Send query and get response\n",
        "    response = client.models.generate_content(\n",
        "        model=MODEL_ID,\n",
        "        config=generation_config,\n",
        "        contents=contents,\n",
        "    )\n",
        "    # Handle tool calls if present\n",
        "    for part in response.candidates[0].content.parts:\n",
        "        contents.append(types.Content(role=\"model\", parts=[part]))\n",
        "        if part.function_call:\n",
        "            function_call = part.function_call\n",
        "            # add the function call to the contents\n",
        "            # Call the tool with arguments\n",
        "            tool_result = await tools[function_call.name][\"callable\"](\n",
        "                **function_call.args\n",
        "            )\n",
        "            # Build the response parts.\n",
        "            function_response_part = types.Part.from_function_response(\n",
        "                name=function_call.name,\n",
        "                response={\"result\": tool_result},\n",
        "            )\n",
        "            contents.append(types.Content(role=\"user\", parts=[function_response_part]))\n",
        "            # Send follow-up with tool results\n",
        "            func_gen_response = client.models.generate_content(\n",
        "                model=MODEL_ID, config=generation_config, contents=contents\n",
        "            )\n",
        "            contents.append(types.Content(role=\"model\", parts=[func_gen_response]))\n",
        "    return contents\n",
        "\n",
        "\n",
        "async def main():\n",
        "    \"\"\"\n",
        "    Main function that sets up the MCP server, initializes tools, and runs the interactive loop.\n",
        "    The server is run in a Docker container to ensure isolation and consistency.\n",
        "    \"\"\"\n",
        "    # Configure Docker-based MCP server for SQLite\n",
        "    server_params = StdioServerParameters(\n",
        "        command=\"docker\",\n",
        "        args=[\n",
        "            \"run\",\n",
        "            \"--rm\",  # Remove container after exit\n",
        "            \"-i\",  # Interactive mode\n",
        "            \"-v\",  # Mount volume\n",
        "            \"mcp-test:/mcp\",  # Map local volume to container pathx\n",
        "            \"mcp/sqlite\",  # Use SQLite MCP image\n",
        "            \"--db-path\",\n",
        "            \"/mcp/test.db\",  # Database file path inside container\n",
        "        ],\n",
        "        env=None,\n",
        "    )\n",
        "\n",
        "    # Start MCP client and create interactive session\n",
        "    async with MCPClient(server_params) as mcp_client:\n",
        "        # Get available database tools and prepare them for the LLM\n",
        "        mcp_tools = await mcp_client.get_available_tools()\n",
        "        # Convert MCP tools into a format the LLM can understand and use\n",
        "        tools = {\n",
        "            tool.name: {\n",
        "                \"name\": tool.name,\n",
        "                \"callable\": mcp_client.call_tool(\n",
        "                    tool.name\n",
        "                ),  # returns a callable function for the rpc call\n",
        "                \"schema\": {\n",
        "                    \"type\": \"function\",\n",
        "                    \"function\": {\n",
        "                        \"name\": tool.name,\n",
        "                        \"description\": tool.description,\n",
        "                        \"parameters\": tool.inputSchema,\n",
        "                    },\n",
        "                },\n",
        "            }\n",
        "            for tool in mcp_tools\n",
        "            if tool.name\n",
        "            != \"list_tables\"  # Excludes list_tables tool as it has an incorrect schema\n",
        "        }\n",
        "\n",
        "        # Start interactive prompt loop for user queries\n",
        "        messages = None\n",
        "        while True:\n",
        "            try:\n",
        "                # Get user input and check for exit commands\n",
        "                user_input = input(\"\\nEnter your prompt (or 'quit' to exit): \")\n",
        "                if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
        "                    break\n",
        "                # Process the prompt and run agent loop\n",
        "                messages = await agent_loop(user_input, tools, messages)\n",
        "                # Find the last model message with text and print it\n",
        "                for message in reversed(messages):\n",
        "                    if message.role == \"model\" and any(\n",
        "                        part.text for part in message.parts\n",
        "                    ):\n",
        "                        for part in message.parts:\n",
        "                            if part.text is not None and part.text.strip() != \"\":\n",
        "                                print(f\"Assistant: {part.text}\")\n",
        "                                break\n",
        "                        break\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nExiting...\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError occurred: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6hOgvdawoLj"
      },
      "outputs": [],
      "source": [
        "#if __name__ == \"__main__\":\n",
        "#    asyncio.run(main())\n",
        "await main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ultimate_styler",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
